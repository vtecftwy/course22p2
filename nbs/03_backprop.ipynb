{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The forward and backward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\n",
    "from pathlib import Path\n",
    "from torch import tensor\n",
    "from fastcore.test import test_close\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "torch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\n",
    "np.set_printoptions(precision=2, linewidth=125)\n",
    "\n",
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f: \n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundations version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,m = x_train.shape\n",
    "c = y_train.max()+1\n",
    "n,m,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a simple model with three layers: \n",
    "- L1: hidden linear layer with `nh` units\n",
    "- L2: relu layer\n",
    "- O: output linear layer with ` unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num hidden units\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the two sets of weight and biases matices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Y = X \\times W + B \\\\ \\updownarrow \\\\\n",
    "\\begin{bmatrix}\n",
    "  y_{0} & y_{1} & \\dots & y_{50}\\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "  x_{0} & x_{1} & \\dots & x_{783}\\\\\n",
    "\\end{bmatrix} \\times\n",
    "\\begin{bmatrix}\n",
    "  w_{0,0}    & w_{0,1}    & \\dots & w_{0, 49} \\\\\n",
    "  w_{1, 0}   & w_{1, 1}   & \\dots & w_{1, 49} \\\\\n",
    "             &            &\\vdots             \\\\\n",
    "  w_{783, 0} & w_{783, 1} & \\dots & w_{783, 49}\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "  b_{0} & b_{1} & \\dots & b_{50}\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 50]), torch.Size([50, 1]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = torch.randn(m,nh)\n",
    "b1 = torch.zeros(nh)\n",
    "w2 = torch.randn(nh,1)\n",
    "b2 = torch.zeros(1)\n",
    "\n",
    "w1.shape, w2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the layers and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b): \n",
    "    \"\"\"Linear layer\"\"\"\n",
    "    return x@w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 50])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = lin(x_valid, w1, b1)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): \n",
    "    \"\"\"Rectified Linear Unit\"\"\"\n",
    "    return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 50])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = relu(t)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(xb):\n",
    "    l1 = lin(xb, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    o = lin(l2, w2, b2)\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model(x_valid)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function: MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Of course, `mse` is not a suitable loss function for multi-class classification; we'll use a better loss function soon. We'll use `mse` for now to keep things simple.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 1]), torch.Size([10000]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape,y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just take the difference between `res` and `y_valid`, we will broadcast in the wrong way:\n",
    "- trailing dimension: 1 in `res` `[10000,1]` compared to 10000 `y_valid` `[10000]` => `res` is broadcasted to `[10000,10000]`\n",
    "- other dimension: 10000 in `res` `[10000,1]` compared to None in `y_valid` `[10000]` => `y_valid`  is broadcasted to `[10000,10000]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 10000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res-y_valid).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get rid of that trailing (,1), in order to use `mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:,0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of doing this is to use `squeeze`; `squeeze` will remove an empty dimension (1 in shape on that axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000]), torch.Size([10000]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res[:,0]-y_valid).shape, (res.squeeze()-y_valid).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, `y_train` and `y_valid` have a dtype `int64` while `res` or `preds` are `float32`. It is a good practice to align both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.int64, torch.float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.dtype, res.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 1]), torch.float32, torch.float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train,y_valid = y_train.float(),y_valid.float()\n",
    "\n",
    "preds = model(x_train)\n",
    "preds.shape, preds.dtype, y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(output, targ): \n",
    "    return (output.squeeze()-targ).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4308.76)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(preds, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients and backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Symbolic derivation !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 x$"
      ],
      "text/plain": [
       "2*x"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import symbols,diff\n",
    "x,y = symbols('x y')\n",
    "diff(x**2, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 6 x$"
      ],
      "text/plain": [
       "6*x"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(3*x**2+9, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lin_grad(inp, out, w, b):\n",
    "#     \"\"\"grad of matmul with respect to input\"\"\"\n",
    "#     inp.g = out.g @ w.t()\n",
    "#     w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "#     b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to build the function for gradient\n",
    "\n",
    "1. Use python debugger to understand what is going on\n",
    "1. Lots of unsqueeze, ... means we probably can use a einsum. check with the debugger\n",
    "1. Replace by a einsum\n",
    "1. Actually, looks like a matmul. check and replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward Pass**:\n",
    "\n",
    "<img src=\"images/backprop.png\" width=\"50%\">\n",
    "\n",
    "Also see this [link](https://nasheqlbrm.github.io/blog/posts/2021-11-13-backward-pass.html) for a post with both math and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "def lin_grad(inp, out, w, b):\n",
    "    # grad of matmul with respect to input\n",
    "#     pdb.set_trace()\n",
    "    inp.g = out.g @ w.t()\n",
    "#     i,o = inp.unsqueeze(-1), out.g.unsqueeze(1)\n",
    "#     w.g = (i * o).sum(0)\n",
    "    w.g = inp.T@out.g\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # forward pass:\n",
    "    l1 = inp @ w1 + b1\n",
    "    l2 = relu(l1)\n",
    "    out = l2 @ w2 + b2\n",
    "\n",
    "    diff = out[:,0] - targ\n",
    "    loss = diff.pow(2).mean()\n",
    "    \n",
    "#     pdb.set_trace()\n",
    "    \n",
    "    # backward pass:\n",
    "    # 1. gradient loss w.r.t out (dLoss/dout) is added as an attribute to the tensor out\n",
    "    out.g = 2. * diff[:,None] / inp.shape[0]\n",
    "\n",
    "    # 2. gradient out (L2 output) w.r.t l2 (L2 input)\n",
    "    lin_grad(l2, out, w2, b2)\n",
    "    \n",
    "    # 3. gradient l2 (Relu output) w.r.t. l1 (L1 output)\n",
    "    l1.g = (l1 > 0).float() * l2.g\n",
    "    \n",
    "    # 4. gradient l1 (L1 output) w.r.t. inputs\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debugger `pdb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Past tracing inside the two functions**\n",
    "```python\n",
    "> /tmp/ipykernel_12530/3466932735.py(14)forward_and_backward()\n",
    "     12     # backward pass:\n",
    "     13     # 1. gradient loss w.r.t out (dLoss/dout) is added as an attribute to the tensor out\n",
    "---> 14     out.g = 2. * diff[:,None] / inp.shape[0]\n",
    "     15 \n",
    "     16     # 2. gradient out (L2 output) w.r.t l2 (L2 input)\n",
    "\n",
    "ipdb> p out.g\n",
    "*** AttributeError: 'Tensor' object has no attribute 'g'\n",
    "ipdb> n\n",
    "> /tmp/ipykernel_12530/3466932735.py(17)forward_and_backward()\n",
    "     15 \n",
    "     16     # 2. gradient out (L2 output) w.r.t l2 (L2 input)\n",
    "---> 17     lin_grad(l2, out, w2, b2)\n",
    "     18 \n",
    "     19     # 3. gradient l2 (Relu output) w.r.t. l1 (L1 output)\n",
    "\n",
    "ipdb> p out.g\n",
    "tensor([[-16.68],\n",
    "        [  9.42],\n",
    "        [-18.07],\n",
    "        [  6.24]])\n",
    "ipdb> c\n",
    "> /tmp/ipykernel_12530/1343070491.py(6)lin_grad()\n",
    "      4     # grad of matmul with respect to input\n",
    "      5     pdb.set_trace()\n",
    "----> 6     inp.g = out.g @ w.t()\n",
    "      7     i,o = inp.unsqueeze(-1), out.g.unsqueeze(1)\n",
    "      8     w.g = (i * o).sum(0)\n",
    "\n",
    "ipdb> p inp.g\n",
    "*** AttributeError: 'Tensor' object has no attribute 'g'\n",
    "ipdb> n\n",
    "> /tmp/ipykernel_12530/1343070491.py(7)lin_grad()\n",
    "      5     pdb.set_trace()\n",
    "      6     inp.g = out.g @ w.t()\n",
    "----> 7     i,o = inp.unsqueeze(-1), out.g.unsqueeze(1)\n",
    "      8     w.g = (i * o).sum(0)\n",
    "      9     w.g = inp.T@out.g\n",
    "\n",
    "ipdb> p inp.g.shape\n",
    "torch.Size([4, 50])\n",
    "ipdb> p out.shape\n",
    "torch.Size([4, 1])\n",
    "ipdb> p out.g.shape\n",
    "torch.Size([4, 1])\n",
    "ipdb> p inp.unsqueeze(-1).shape\n",
    "torch.Size([4, 50, 1])\n",
    "ipdb> p out.g.unsqueeze(-1)\n",
    "tensor([[[-16.68]],\n",
    "\n",
    "        [[  9.42]],\n",
    "\n",
    "        [[-18.07]],\n",
    "\n",
    "        [[  6.24]]])\n",
    "ipdb> p out.g.unsqueeze(-1).shape\n",
    "torch.Size([4, 1, 1])\n",
    "ipdb> n\n",
    "> /tmp/ipykernel_12530/1343070491.py(8)lin_grad()\n",
    "      6     inp.g = out.g @ w.t()\n",
    "      7     i,o = inp.unsqueeze(-1), out.g.unsqueeze(1)\n",
    "----> 8     w.g = (i * o).sum(0)\n",
    "      9     w.g = inp.T@out.g\n",
    "     10     b.g = out.g.sum(0)\n",
    "\n",
    "ipdb> p i.shape,o.shape\n",
    "(torch.Size([4, 50, 1]), torch.Size([4, 1, 1]))\n",
    "ipdb> (i*o).shape\n",
    "torch.Size([4, 50, 1])\n",
    "ipdb> (inp.T@out.g).shape\n",
    "torch.Size([50, 1])\n",
    "ipdb> (i * o).sum(0).shape\n",
    "torch.Size([50, 1])\n",
    "ipdb> q\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ipdb> h\n",
    "\n",
    "Documented commands (type help <topic>):\n",
    "========================================\n",
    "EOF    commands   enable    ll        pp       s                until \n",
    "a      condition  exit      longlist  psource  skip_hidden      up    \n",
    "alias  cont       h         n         q        skip_predicates  w     \n",
    "args   context    help      next      quit     source           whatis\n",
    "b      continue   ignore    p         r        step             where \n",
    "break  d          interact  pdef      restart  tbreak         \n",
    "bt     debug      j         pdoc      return   u              \n",
    "c      disable    jump      pfile     retval   unalias        \n",
    "cl     display    l         pinfo     run      undisplay      \n",
    "clear  down       list      pinfo2    rv       unt            \n",
    "\n",
    "Miscellaneous help topics:\n",
    "==========================\n",
    "exec  pdb\n",
    "\n",
    "ipdb> p\n",
    "*** SyntaxError: invalid syntax\n",
    "ipdb> p inp.shape\n",
    "torch.Size([50000, 50])\n",
    "ipdb> print(inp.shape)\n",
    "torch.Size([50000, 50])\n",
    "ipdb> p out.g.shape\n",
    "torch.Size([50000, 1])\n",
    "ipdb> p (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0).shape\n",
    "torch.Size([50, 1])\n",
    "ipdb> p (inp.unsqueeze(-1) * out.g.unsqueeze(1)).shape\n",
    "torch.Size([50000, 50, 1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 50]) torch.Size([784, 50])\n",
      "torch.Size([50]) torch.Size([50])\n",
      "torch.Size([50, 1]) torch.Size([50, 1])\n",
      "torch.Size([1]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(w1.shape, w1.g.shape)\n",
    "print(b1.shape, b1.g.shape)\n",
    "print(w2.shape, w2.g.shape)\n",
    "print(b2.shape, b2.g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for testing against later\n",
    "def get_grad(x): return x.g.clone()\n",
    "chks = w1,w2,b1,b2,x_train\n",
    "grads = w1g,w2g,b1g,b2g,ig = tuple(map(get_grad, chks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cheat a little bit and use PyTorch autograd to check our results.\n",
    "- we create a set of tensors with autograd w12,w22,b12,b22,xt2 with same values as w1,w2,b1,b2,x_train\n",
    "- we make a forward pass, and pytorch calculates the grads automatically\n",
    "- we compare the two sets of gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkgrad(x): return x.clone().requires_grad_(True)\n",
    "ptgrads = w12,w22,b12,b22,xt2 = tuple(map(mkgrad, chks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_loss(inp, targ):\n",
    "    l1 = inp @ w12 + b12\n",
    "    l2 = relu(l1)\n",
    "    out = l2 @ w22 + b22\n",
    "    return mse(out, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = forward_and_loss(xt2, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w22.grad, w2g, eps=0.01)\n",
    "test_close(b22.grad, b2g, eps=0.01)\n",
    "test_close(w12.grad, w1g, eps=0.01)\n",
    "test_close(b12.grad, b1g, eps=0.01)\n",
    "test_close(xt2.grad, ig , eps=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in zip(grads, ptgrads): \n",
    "    test_close(a, b.grad, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code is rather clunky above. We will refactor\n",
    "1. layer as classes, \n",
    "    - with a `__call__` method to make them callable. \n",
    "    - `__call__` will not only calculate the output but also will store input and outputs for the backward pass. this avoids to compute all outputs twice, at the expense of memory\n",
    "    - with a backward method for backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self): \n",
    "        self.inp.g = (self.inp > 0).float() * self.out.g\n",
    "\n",
    "# self.out.g does not exist at the time of forward pass, but will be set by the backward pass of the following layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin():\n",
    "    # __init__ because we need to initialize the weights upon layer creation\n",
    "    def __init__(self, w, b): self.w, self.b = w,b\n",
    "        \n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp@self.w + self.b\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = self.inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse():\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp = inp\n",
    "        self.targ = targ\n",
    "        self.out = (inp.squeeze() - targ).pow(2).mean()\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: \n",
    "            x = l(x)\n",
    "        self.preds = x\n",
    "        self.final_loss = self.loss(x, targ)\n",
    "        return self.final_loss\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): \n",
    "            l.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this model, the loss function is in the model (used often for huggingface models).\n",
    ">\n",
    ">In fastai, it is often outside and calculate separetely.\n",
    ">\n",
    ">Both are fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4308.76)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model(x_train, y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 1]), tensor(4308.76))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.preds.shape, model.final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 50]), torch.Size([784, 50]), tensor(151.95), tensor(-70.73))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].w.shape, model.layers[0].w.g.shape, model.layers[0].w.g.max(), model.layers[0].w.g.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w2g, w2.g, eps=0.01)\n",
    "test_close(b2g, b2.g, eps=0.01)\n",
    "test_close(w1g, w1.g, eps=0.01)\n",
    "test_close(b1g, b1.g, eps=0.01)\n",
    "test_close(ig, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better, but still have repeated code, e.g. the `__call__` method in each layer. \n",
    "\n",
    "Let's have a template as `Module`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of `Module`\n",
    "- `__call__` template method for all modules, like the one in each layer, but generalized to takes any list of positional args (inputs), save them, run a forward pass, and save the results\n",
    "- `forward` depends on each specific layer\n",
    "- `backward` is the same for all module, but calls a specific method `bwd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "\n",
    "    def forward(self): raise Exception('not implemented')\n",
    "    def backward(self): self.bwd(self.out, *self.args)\n",
    "    def bwd(self): raise Exception('not implemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplifies the layers. For instance, previous Relu was:\n",
    "```python\n",
    "class Relu():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self): \n",
    "        self.inp.g = (self.inp>0).float() * self.out.g\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def forward(self, inp): return inp.clamp_min(0.)\n",
    "    def bwd(self, out, inp): \n",
    "        inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "    def forward(self, inp): \n",
    "        return inp@self.w + self.b\n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse(Module):\n",
    "    def forward (self, inp, targ): \n",
    "        self.diff = (inp.squeeze() - targ)\n",
    "        return self.diff.pow(2).mean()\n",
    "        \n",
    "    def bwd(self, out, inp, targ): \n",
    "        inp.g = 2*(self.diff).unsqueeze(-1) / targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w2g, w2.g, eps=0.01)\n",
    "test_close(b2g, b2.g, eps=0.01)\n",
    "test_close(w1g, w1.g, eps=0.01)\n",
    "test_close(b1g, b1.g, eps=0.01)\n",
    "test_close(ig, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we may use the pytorch package. And pytorch calculates the gradient itself, no need to define `backward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.w = torch.randn(n_in,n_out).requires_grad_()\n",
    "        self.b = torch.zeros(n_out).requires_grad_()\n",
    "    def forward(self, inp): return inp@self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [Linear(n_in,nh), nn.ReLU(), Linear(nh,n_out)]\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return F.mse_loss(x, targ[:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 1)\n",
    "loss = model(x_train, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-19.60,  -2.40,  -0.12,   1.99,  12.78, -15.32, -18.45,   0.35,   3.75,  14.67,  10.81,  12.20,  -2.95, -28.33,\n",
       "          0.76,  69.15, -21.86,  49.78,  -7.08,   1.45,  25.20,  11.27, -18.15, -13.13, -17.69, -10.42,  -0.13, -18.89,\n",
       "        -34.81,  -0.84,  40.89,   4.45,  62.35,  31.70,  55.15,  45.13,   3.25,  12.75,  12.45,  -1.41,   4.55,  -6.02,\n",
       "        -62.51,  -1.89,  -1.41,   7.00,   0.49,  18.72,  -4.84,  -6.52])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l0 = model.layers[0]\n",
    "l0.b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
